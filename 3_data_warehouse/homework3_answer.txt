Preparations:
- updated terraform to make the homework bucket
- downloaded indicated data files using airflow
- created homework bq external tables
- created homework bg regular tables


1.
Query: SELECT COUNT(*)/1000000 FROM `dtc-de-course-462612.trips_data_all.homework3_external_table` LIMIT 1000;

Answer: 20,332,093


2.
-- For external table: data read estimated amount 0 MB, actual 155.12 MB 
SELECT COUNT(DISTINCT(PULocationID)) FROM `trips_data_all.homework3_external_table` LIMIT 10;

-- For regular non-clustered, non-partitioned table: data read est. 155.12 MB, actual 155.12 MB
SELECT COUNT(DISTINCT(PULocationID)) FROM `trips_data_all.homework3_table` LIMIT 10;

Answer: 0 MB for the External Table and 155.12 MB for the Materialized Table


3.
SELECT PULocationID FROM `trips_data_all.homework3_table`;
-- est. read: 155.12 MB

SELECT PULocationID, DOLocationID FROM `trips_data_all.homework3_table`;
-- est. read: 310.24 MB

-- The 2nd query 2 columns so it needs to scan and read 2 columns of data compared to only 1 in the first query.

Answer: BigQuery is a columnar database, and it only scans the specific columns requested in the query. 
Querying two columns (PULocationID, DOLocationID) requires reading more data than querying 
one column (PULocationID), leading to a higher estimated number of bytes processed.


4.
SELECT COUNT(*)
FROM `trips_data_all.homework3_table`
WHERE fare_amount = 0;

Answer: 8,333


5. 
Partition by tpep_dropoff_datetime and Cluster by VendorID

CREATE OR REPLACE TABLE `trips_data_all.homework3_partitioned_clustered_table`
PARTITION BY DATE(tpep_dropoff_datetime)
CLUSTER BY VendorID AS
SELECT VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge
FROM `trips_data_all.homework3_external_table`;

Answer: Partition by tpep_dropoff_datetime and Cluster on VendorID

6. 
SELECT tpep_dropoff_datetime
FROM `trips_data_all.homework3_partitioned_clustered_table`
WHERE DATE(tpep_dropoff_datetime) BETWEEN '2024-03-01' AND '2024-03-15'
ORDER BY tpep_dropoff_datetime DESC 
LIMIT 100;

SELECT DISTINCT(VendorID)
FROM `trips_data_all.homework3_table`
WHERE DATE(tpep_dropoff_datetime) BETWEEN '2024-03-01' AND '2024-03-15';
-- est. read: 310.24 MB

SELECT DISTINCT(VendorID)
FROM `trips_data_all.homework3_partitioned_clustered_table`
WHERE DATE(tpep_dropoff_datetime) BETWEEN '2024-03-01' AND '2024-03-15';
-- est. read 26.84 MB

Answer: 310.24 MB for non-partitioned table and 26.84 MB for the partitioned table


7.
Answer: GCP Bucket


8.
It depends if your data size is at least 1 GBs. 

Answer: False

-- Bonus
SELECT COUNT(*)
FROM `trips_data_all.homework3_table`;
-- est. read: 0 MB, since bigquery stores tables by column, it needs a specific column name to be able to estimate the read.

Solution Answer: Bigquery caches metadata for tables that includes number of bytes per column which is 
how it is able to give estimates. If the column is not specific, it won't be able to read it from 
the metadata.